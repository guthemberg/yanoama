Caju's road map:

IMPORTANT NOTE: to fix problem with yum on planetlab nodes run
sudo sed -i 's/https/http/g' /etc/yum.repos.d/*

#IMPORTANT NOTE ABOUT CRON
#edit /etc/pam.d/crond to fix problem with cron, its content must be as follows
#
# The PAM configuration file for the cron daemon
#
#
# No PAM authentication called, auth modules not needed
auth required pam_rootok.so
account required pam_localuser.so
session required pam_loginuid.so 
#account    required   pam_access.so
#account    include    password-auth
#session    required   pam_loginuid.so
#session    include    password-auth
#auth       include    password-auth

* Overview: this design is based on caju [Silvestre et al. 2012] and 
murder (a bittorrent implementation). From caju, we have the concept 
of: 
	+ storage domain: a set of storage peers that are close to each 
	other, for instance, latency measurement
	+ peers' roles:
		- coordinator: server that guides storage nodes in terms of 
		resources mapping and accounting. bittorrent seeders run on 
		this node, as well monitoring, including amen.
		- storage node/peer: peers that share and view videos. they 
		compose a hybrid CDN for video distribution.

* Basic scheme:
	 _______________                 _______________
	|               |   commands    |               | 
	| coordinator   |-------------->| storage node  |----
	| (pilot client,|               | (pilot server,|    |
	| seeder,       |               | peer, amen    |    |
	| amen monitor: |  measurements | agent/deamon) |<---
	| mongobd)      |<--------------|               | view/share
	|_______________|               |_______________|

* yanoama dev project (brief) description:
	+ README file (this one): overall description
	+ config: configuration files
		- yanoama.conf : main conf file
			* coordinator : nodes that play the role of coordinators. 
			entries: hostname, ip, latencies (that defines membership 
			to a storage domain)
			* hosts: particular nodes in the system, e.g. localhost, 
			european monitor....
			* pilot: configurations for pilot, port:port_number
		- ple.conf : ple configurations for fetching  information from
		API
			* API information: username, password, and host 
	+ contrib: main daemons for running yanoama module
		- yanoama: main yanoama deamons and client tools
			* pilotd and yanoamad
	+ yanoama (python module): main module
		- amen: adaptive monitoring for edge networks is twofold: 
		mongobd server for coordinator and a agent/deamon for storage 
		nodes/peers. further information in its README.txt 
		- monitoring: coordinators run a couple of scripts for 
		maintaining storage domain membership
		- pilot: this server runs on peers and allows coordinators to 
		send commands to them (create video, see video...)
		- planetlab: module with common procedured to interact with 
		planetlab API
		- system: main systems scripts, for bootstrapping, install 
		cron, /etc/hosts file  

* step-by-step deployment based on role:
	+ coordinator: install MongoD

* db scheme (main collections)
	+ yanoama.nodes_latency_measurements: raw latency measurements for
	membership. it contains nodes that are closer to this coordinator,
	according to the latency measurement. note that key are hostnames 
	with colons instead of dots 
	
* this is draft for the new deployment in planetlab
	+ yanoama/planetlab/planetlab.py: library for planetlab API
	+ yanoama/planetlab/update_slice.py: updates 'expires' time for 
	eight more weeks
	+ yanoama/monitoring/get_rtt.py: main script for getting rtt from 
	a planetlab nodes based on a list of destinations nodes (e.g. 
	ple_nodes.pck)
	+ yanoama/system/get_rtt.sh: sript that runs ping. five icmp probe
	packets are sent to measure rtt
	
some scripts, notably update_slice.py, require a ple.conf file with planetlab account credentials in the following format:
#config/ple.conf: information for PlanetLab API
username = email
password = password
host=api_hostname

#more docs





##INSTALL pilot module

Pilot - A simple remote console commands runner for aren/planetlab
----------------


(I) How to install the server on planetlab nodes (assuming upmc-aren slice, and homedir /home/upmc_aren).

get pilot.tar.gz (with this README.txt, pilot python package, config and contrib folders.) 
and copy it into /home/upmc_aren

cd /home/upmc_aren && #get yanoama code# && cd yanoama

sudo mkdir /usr/local/yanoama && sudo touch /usr/local/yanoama/pilot.log

sudo cp yanoama/config/yanoama.conf /etc/

cp contrib/pilot/pilotd ./

chmod +x pilotd

sudo ./pilotd start (for stopping /home/upmc_aren/pilot/pilotd stop)

----------------


(II) running the client (for example, for workload master)

get pilot.tar.gz (with this README.txt, pilot python package, config and contrib folders.) and copy into /home/upmc_aren

cd /home/upmc_aren && tar xzf pilot.tar.gz && cd pilot

sudo cp config/pilot.conf /etc/

cp contrib/pilot/client.py ./

python client.py HOST CMD, e.g. get_date

----------------


(III) Packaging (example)
cd /tmp
tar -C  ~/Documents/workplace/ -cf pilot.tar pilot
gzip pilot.tar
scp pilot.tar.gz upmc_aren@plewifi.ipv6.lip6.fr:/home/upmc_aren/
